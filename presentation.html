<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Workflow Engines Up and Running</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/sky.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <style>
        #left {
            left: -8.33%;
            text-align: left;
            float: left;
            width: 40%;
            z-index: -10;
        }

        #right {
            left: -10%;
            top: 75px;
            float: right;
            text-align: right;
            z-index: -10;
            width: 40%;
        }

        .reveal pre code {
            font-size: 1.5em;
            line-height: 1.3;
        }

        .reveal pre {
            width: 120%;
            margin-left: -10%;
            height: 80%;
        }

        .reveal p { 
            text-align: left;
            font-size: 80%;
        }
    </style>
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section>
            <h2>Setup Instructions</h2>
            <p>
                <ul>
                    <li>Airflow - TODO</li>
                    <li>Luigi - TBD</li>
                </ul>
            </p>
        </section>
        <section>
            <div id="left">
                <b>Ian Zelikman</b>
                <ul>
                    <li>Python dev for 6+ years</li>
                    <li>Backend web services</li>
                    <li>ETL Process</li>
                    <br/>
                    @izcoder<br/>
                    ian.zelikman@gmail.com
                </ul>
            </div>
            <div id="right">
                <b>Austin Hacker</b>
                <ul>
                    <li></li>
                    <li></li>
                </ul>
            </div>
        </section>
        <section>
            <h1>Intro</h1>
            <aside class="notes">
The idea for this tutorial came as Austin and I were looking at different solutions to ETL jobs that
our team.<br>
The intention is for developers that might be considering using these tools to get some more hands on experience<br>
with them. More than just reading a tutorial but experiment with writing DAGs for them, see things fail<br>
and fix them and have a feeling where these tools excel and where they come short.<br>
Also as many of you might be using Python frameworks as we do these tools will be a natural fit.<br>
            </aside>
        </section>
        <section>
            <h2>Structure</h2>
            <aside class="notes">
We hope you had opportunity to download the repos and install the requirements. If not you will have
Have some additional time to do this.<br>
The tutorial is split into two sections. In each section we will present the engine we are working with.<br>
We will then have a series of lessons where in each lesson we will cover some theory and then you can practice<br>
            </aside>
        </section>
        <section>
            <h2>Airflow</h2>
            <ul>
                <li>Originally by Airbnb</li>
                <li>"programmatically author, schedule and monitor workflows"</li>
                <li>Distributed</li>
                <li>Not for data streaming</li>
            </ul>
            <aside class="notes">
It was developed originally by Maxime Beauchemin at Airbnb. History: started in 2014, brought to Airbnb github in 2015.<br>
Moved to Apache incubation (currently) in 2016.<br>
By Authoring DAGs using python you will get great flexibility. With that Airflow gives you many features such as rich UI and scheduling tools.<br>
As we will see was designed with multiple components that enable it to be very scalable and stable.<br>
This is not a data streaming solution (Spark streaming, storm), similar solutions are Pinball, Oozie, Azkaban, and Luigi.
            </aside>
        </section>
        <Section>
            <h2>Airflow Components</h2>
            <ul>
                <li>UI</li>
                <li>MetaData Database(Postgres)</li>
                <li>Scheduler</li>
                <li>Executor</li>
                <li>Workers</li>
            </ul>
            <aside class="notes">
Airflow UI is a Flask application that is used to display, monitor and control our DAG execution.<br>
Metadata database is used to store information about the current state of DAGs and tasks to facilitate UI presentation and scheduler operation<br>
The scheduler is a python process that runs and is used to schedule DAG and tasks runs.<br>
The executor is a component tightly coupled to the scheduler and triggers units of work on the workers. You can have local vs. celery executes.<br>
The worker actually execute the python code that was defined.
            </aside>
        </Section>
        <Section>
            <h2>Airflow Demo</h2>
        </Section>
        <aside class="notes">
We will show the Airflow UI through which we will also show the components, functionality and the docker<br>
implementation you will use.
        </aside>
        <section>
            <h2>Lesson 1</h2>
            <p>
                Download and follow instructions at: TODO<br><br>
                Verify all components of Airflow are working and you can run the sample DAG.<br><br>
                Explore the UI and see information about the DAG run.<br><br>
                <b>Bonus</b>: Update DAG code so that there are errors during a task run. See details of failed DAG run.
            </p>
        </section>
        <section>
            <h4>Airflow DAGs</h4>
            <pre><code data-trim data-noescape>
                default_args = {
                    'owner': 'pytn',
                    'depends_on_past': False,
                    'start_date': datetime(2015, 6, 1),
                    'email': ['airflow@example.com'],
                }
                dag = DAG('tutorial', default_args=default_args)
            </code></pre>
        </section>
        <section>
            <h4>Airflow Tasks - Bash Operator</h4>
            <pre><code data-trim data-noescape>
                t1 = BashOperator(
                    task_id='print_date',
                    bash_command='date',
                    dag=dag)
            </code></pre>
            <pre><code data-trim data-noescape>
                 t2 = BashOperator(
                    task_id='sleep',
                    bash_command='sleep 5',
                    dag=dag)

                t2.set_upstream(t1)
            </code></pre>
        </section>
        <section>
            <h4>Airflow Tasks - Python Operator</h4>
            <pre><code data-trim data-noescape>
                def print_context(ds, **kwargs):
                    pprint(kwargs)
                    print(ds)
                    return 'Whatever you return gets printed in the logs'
            </code></pre>
            <pre><code data-trim data-noescape>
                run_this = PythonOperator(
                    task_id='print_the_context',
                    provide_context=True,
                    python_callable=print_context,
                    dag=dag)
            </code></pre>
        </section>
        <section>
            <h2>Lesson 2</h2>
            <p>
                <b>Construct a DAG with multiple tasks.</b><br><br>
                Download zip file with most common names:<br>
                https://www.ssa.gov/oact/babynames/names.zip<br><br>
                Find the most common name
            </p>
        </section>
        <section>
            <h2>Advanced DAGs</h2>
        </section>
        <section>
            <h3>Retries</h3>
            <pre><code data-trim data-noescape>
                dag_args = {
                    'email_on_retry': False,
                    'retries': 3,
                    'retry_delay': timedelta(seconds=30),
                }
            </code></pre>
            <pre><code data-trim data-noescape>
                BashOperator(
                    task_id='sleep',
                    bash_command='sleep 5',
                    retries=3,
                    dag=dag)
            </code></pre>
        </section>
        <section>
            <h2>Jinja2</h2>
            <pre><code data-trim data-noescape>
                templated_command = """
                {% for i in range(5) %}
                echo "{{ ds }}"
                echo "{{ macros.ds_add(ds, 7) }}"
                echo "{{ params.my_param }}"
                {% endfor %}
                """
            </code></pre>
            <pre><code data-trim data-noescape>
                BashOperator(
                    task_id='templated',
                    bash_command=templated_command,
                    params={'my_param': 'Parameter I passed in'},
                    dag=dag)
            </code></pre>
        </section>
        <section>
            <h2>Parallel Execution</h2>
            <pre><code data-trim data-noescape>
                for i in range(5):
                    BashOperator(
                        task_id='task-num-' + i,
                        bash_command=script.sh,
                        dag=dag)
            </code></pre>
        </section>
        <section>
            <h2>Lesson 3</h2>
            <p>
                <b>Part 1</b><br>
                Construct a DAG that downloads a zip file of common names by state
                https://www.ssa.gov/oact/babynames/state/namesbystate.zip<br><br>

                Calculate the most common names in 5 states.<br>
                Make the calculation as parallel as possible.<br><br>

                <b>Part 2</b><br>
                Use  <b>pytn_utils.RandomFailOpen</b> instead the built in open<br>
                Add retries to make sure your DAG succeeds.
            </p>
        </section>
        <section>
            <h2>More Airflow features</h2>
            <ul>
                <li>Pools</li>
                <li>SLA</li>
                <li>Historic Visualization</li>
                <li>Xcom</li>
            </ul>
        </section>
        <section>
            <h2>Testing</h2>
            <ul>
                <li>Unit Tests</li>
                <li>Dev/QA Environment</li>
            </ul>
        </section>
        <section>
            <h2>CLI</h2>
            <pre><code data-trim data-noescape>
                python dags/first_dag.py
            </code></pre>
            <pre><code data-trim data-noescape>
                # command layout: command subcommand dag_id task_id date

                # testing print_date
                airflow test first_dag test_task 2001-01-01
            </code></pre>
        </section>
        <section>
            <h2>Unit Tests</h2>
            <ul>
                <li>Internal Logic</li>
                <li>Operators</li>
            </ul>
        </section>
        <section>
            <h3>Unit Test - Operators</h3>
            <pre><code data-trim data-noescape>
                from airflow.models import BaseOperator
                from airflow.utils.decorators import apply_defaults
            </code></pre>

            <pre><code data-trim data-noescape>
                class PyTNOperator(BaseOperator):
                    @apply_defaults
                    def __init__(self, my_operator_param,
                            *args, **kwargs):
                        self.operator_param = my_operator_param
                        super(MyFirstOperator, self).__init__(
                            *args, **kwargs)
                    def execute(self, context):
                        print("PyTN2018")
            </code></pre>
        </section>
        <section>
            <h3>Unit Test - Operators</h3>
            <pre><code data-trim data-noescape>
                from airflow.models import DAG, TaskInstance
                from test_dag import MyOperator
            </code></pre>

            <pre><code data-trim data-noescape>
                class TestMyOperator(TestCase):
                    def test_execute(self):
                        dag = DAG(dag_id='pytn_dag', start_date=datetime.now())
                        task = MyOperator(dag=dag, task_id='first_task')
                        ti = TaskInstance(task=task, execution_date=datetime.now())
                        result = task.execute(ti.get_template_context())
                        self.assertEqual(result, 'PyTN2018')
            </code></pre>
        </section>
        <section>
            <h2>Dev/QA Server</h2>
        </section>
        <section>
            <h2>Lesson 4</h2>
            <p>
                Use the Airflow CLI to test a task.<br><br>

                Write unit tests for methods or operators in your DAG.
                (TODO: How to sh into the worker)<br><br>

                Update your DAG so you can run it both in production and on QA server.
            </p>
        </section>
        <section>
            <h2>Airflow Summary</h2>
            <div id="left">
                <b></b>
                <ul>
                    <li>Many Features</li>
                    <li>Robust</li>
                    <li>Scalable</li>
                </ul>
            </div>
            <div id="right">
                <ul>
                    <li>DAGs on schedule</li>
                    <li>Limited on full DAG testing</li>
                </ul>
            </div>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
    // More info https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        history: true,

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/notes/notes.js', async: true},
            {
                src: 'plugin/highlight/highlight.js', async: true, callback: function () {
                    hljs.initHighlightingOnLoad();
                }
            }
        ]
    });
</script>
</body>
</html>
